{
 "cells": [
  {
   "attachments": {
    "351cec3c-aebb-4b8f-9a35-1011aa689695.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "ed9d52a3-9617-426a-ab32-3fdfe148976e",
   "metadata": {},
   "source": [
    "**LoRA** makes a small change in the fine-tuning process. Instead of computing gradients for all the entries in W (which is huge), and updating all the parameters, it’s algorithm works as follows:\n",
    "- Freeze the original parameters W at the start of fine-tuning.\n",
    "- Decomposes W into two smaller matrices A and B, and update these two small matrices during fine-tuning. The decomposed matrices are significantly smaller in size than the original weight matrix W, and can easily fit into the memory. The decomposition is done in such a way that we can regenerate W from A and B. Basically A and B are LD, so the resulting LI matrix (That it is W) can be composed from A and B\n",
    "- Once fine-tuning is completed, the smaller matrices A and B get adapted for the specific task.\n",
    "- The important thing to note here is that the pretrained weights are not changed during this process as they were frozen by LoRA. The question arises now that how do we use the adapted matrices A and B to do the inference on a new input. Previously with normal finetuning/pretraining, if input to the model was x, the output of layer used to be W.x. But after fine-tuning the model with LoRA, the updated inference equation is:\n",
    "\n",
    "![image.png](attachment:351cec3c-aebb-4b8f-9a35-1011aa689695.png)\n",
    "\n",
    "where 𝑥 is the input, 𝑊 represents model parameters, 𝐵 and 𝐴 represent low-rank adapters trained during LoRA fine-tunin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1661030-b75f-43b7-9b5e-6e086340381d",
   "metadata": {},
   "source": [
    "The amount of memory saved depends on the rank r, a hyperparameter. For instance, if ΔW has 10,000 rows and 20,000 columns, it holds 200,000,000 parameters. Choosing A and B with r=8, A would have 10,000 rows and 8 columns, and B would have 8 rows and 20,000 columns, resulting in 10,000×8 + 8×20,000 = 240,000 parameters, which is approximately 830 times fewer than 200,000,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5e5c9d-2457-476b-9dba-ad39e43aae66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3bfb95-591b-4ed6-a231-423371bfcbf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45a839a-9e9a-4de1-939d-0f95461114f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8f161f-72b2-4e92-98b8-a731c6c548b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments, TextStreamer\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import triton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d19875d-8391-4ec9-99ae-fcf6f0d1473d",
   "metadata": {},
   "source": [
    "### Get env Ready\n",
    "Lets set the bucket name and model name where you have all your datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b7a0f4-a1a8-41c9-8d73-8b8feaca2e01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Enter the bucket and model name\n",
    "bucket_name = input(\"Enter bucket name:\")\n",
    "model_name = input(\"Enter model name:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33feb1f8-51e1-46cc-b752-dfe44e465969",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "\n",
    "print(f\"{bcolors.HEADER} ---ENV REPORT-- {bcolors.ENDC}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"{bcolors.OKGREEN} CUDA {bcolors.ENDC} cores \")\n",
    "    print(f\"{bcolors.BOLD} GPUs: {torch.cuda.device_count()} {bcolors.ENDC}\\n\")\n",
    "else:\n",
    "    print(f\"{bcolors.OKBLUE} Using CPU cores {bcolors.ENDC}\")\n",
    "\n",
    "print(f\"{bcolors.OKCYAN}LIBRARIES{bcolors.ENDC}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Triton version: {triton.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"scikit-learn version: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e50f67-b8ba-4d66-a219-27ce05d36b4d",
   "metadata": {},
   "source": [
    "### The rank in LoRa Fine tuning\n",
    "A higher rank means a greater number of trainable parameters in our model, making fine-tuning more memory intensive. However, higher ranks retain more information from the original weight matrix, as the decomposed matrices themselves are large and capture most of the essence of W (i.e., the model becomes more expressive). We can say that, as the rank increases, LORA essentially converges toward normal fine-tuning.\n",
    "### Alpha in LoRa\n",
    "A higher “alpha” would place more emphasis on the low-rank structure or regularization, while a lower “alpha” would reduce its influence, making the model rely more on the original parameters. Adjusting “alpha” helps in striking a balance between fitting the data and preventing overfitting by regularizing the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec85936-8e3f-4f47-b817-2bc6fcb03dbb",
   "metadata": {},
   "source": [
    "As a rule of thumb, it’s usually common to choose an alpha that is twice as large as the rank when fine-tuning LLMs (note that this is different when working with diffusion models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce0bd2b-c04e-4e91-a009-c8f21db4bb47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## LoRa\n",
    "rank= 8\n",
    "alpha= 16\n",
    "dropout=0\n",
    "\n",
    "## Model\n",
    "model_id = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\" ## Chose your preferred model\n",
    "\n",
    "max_seq_length = 2048\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_id,\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=True,\n",
    "    dtype=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45afa8d-280c-4f97-8350-7e7164f39a9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the configuration with colors\n",
    "print(f\"{bcolors.HEADER}## LoRa{bcolors.ENDC}\")\n",
    "print(f\"{bcolors.OKGREEN}rank{bcolors.ENDC} = {bcolors.OKBLUE}{rank}{bcolors.ENDC}\")\n",
    "print(f\"{bcolors.OKGREEN}alpha{bcolors.ENDC} = {bcolors.OKBLUE}{alpha}{bcolors.ENDC}\\n\")\n",
    "\n",
    "print(f\"{bcolors.HEADER}## Model{bcolors.ENDC}\")\n",
    "print(f\"{bcolors.OKGREEN}model_id{bcolors.ENDC} = {bcolors.OKBLUE}{model_id}{bcolors.ENDC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c4b0bf-0f2c-457d-b56f-c2db2863b42f",
   "metadata": {},
   "source": [
    "### Test the model without training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cdee4e-a25c-407a-ad5d-553d245e006f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=rank,\n",
    "    lora_alpha=alpha,\n",
    "    lora_dropout=dropout,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"], \n",
    "    use_rslora=True,\n",
    "    use_gradient_checkpointing=\"unsloth\"\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"human\", \"content\": \"Which is the best wine for fishes??\"},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(inputs, streamer = text_streamer, max_new_tokens = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b581dd-1a3d-4f85-a5a2-728b104e3236",
   "metadata": {},
   "source": [
    "### Templating for tokenization\n",
    "When training the model we **must** set a template or use a custom one. In case we want to use a custom one feel free to do so, otherwise we can use one already established. For this iteration as we are using ```meta-llama/Meta-Llama-3.1-8B-Instruct``` we are using the instruct method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417d7bb2-1f1f-4853-94fa-53446c59c8c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "s3_endpoint = os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "s3_secret = os.environ[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "s3_access_key = os.environ[\"AWS_ACCESS_KEY_ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ef224b-1378-4080-98e3-907b313f58c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Create a dropdown widget\n",
    "data_type_widget = widgets.Dropdown(\n",
    "    options=['instructlab', 'labelstudio'],\n",
    "    value=None,\n",
    "    description='Dataset Format:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Display the dropdown\n",
    "display(data_type_widget)\n",
    "\n",
    "# Function to handle the selection\n",
    "def on_change(change):\n",
    "    global data_type\n",
    "    data_type = change['new']\n",
    "    print(f\"Selected format: {data_type}\")\n",
    "\n",
    "# Attach the function to the widget\n",
    "data_type_widget.observe(on_change, names='value')\n",
    "\n",
    "# Initial value of data_type\n",
    "data_type = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca464a5c-b918-4bef-8a81-88f856f1bf35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if data_type == None:\n",
    "    raise Exception(\"No format for dataset has been setted\") \n",
    "print(f\"Using dataset with format: {data_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997c24dc-aaa9-4186-ab5e-33fc0fceaa92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if s3_access_key is None: raise TypeError(f\"'S3_ACCESS_KEY_ID' env variable is not set\")\n",
    "if s3_secret is None: raise TypeError(f\"'S3_SECRET_ACCESS_KEY' env variable is not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc421e7-f126-44b2-8b3a-f602af00ce1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if bucket_name == \"\": raise TypeError(f\"'bucket_name' input variable is empty\")\n",
    "if model_name == \"\": raise TypeError(f\"'model_name' input variable is empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d564ef7-6a86-4c13-9f52-b2d867b3d301",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},\n",
    "    chat_template=\"chatml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dfcd23-a3ee-4417-9877-9e8ca7443459",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Load datasets\n",
    "import boto3\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "\n",
    "session = boto3.session.Session()\n",
    "s3_client = session.client('s3',\n",
    "                           region_name='nyc3',\n",
    "                           endpoint_url=s3_endpoint,\n",
    "                           aws_access_key_id=s3_access_key,\n",
    "                           aws_secret_access_key=s3_secret\n",
    "                          )\n",
    "\n",
    "def load_instruct_lab():\n",
    "    ## Load under dataset variable\n",
    "    paginator=s3_client.get_paginator('list_objects_v2')\n",
    "    pages=paginator.paginate(Bucket=bucket_name,Prefix=f\"datasets/labeled/instructlab/{model_name}\")\n",
    "    i=0\n",
    "    dataset=[]\n",
    "    for page in pages:\n",
    "        page_objects = page.get('Contents', [])\n",
    "        for obj in tqdm(page_objects,desc=\"Loading instructlab jsonl\"):\n",
    "            data = s3_client.get_object(Bucket=bucket_name, Key=obj.get('Key'))\n",
    "            content =(data['Body'].read()).decode(\"utf-8\")\n",
    "            if(i>0 and type(content) is str):\n",
    "                jsonl = json.loads(content)\n",
    "                for row in jsonl:\n",
    "                    user_content= row[\"instruction\"]\n",
    "                    assistant_content=row[\"output\"]\n",
    "                    dataset.append({\n",
    "                        \"messages\":[\n",
    "                            {\n",
    "                                \"value\":user_content,\n",
    "                                \"from\":\"human\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"value\":assistant_content,\n",
    "                                \"from\":\"gpt\"\n",
    "                            }\n",
    "                        ],\n",
    "                        \"prompt_id\":i,\n",
    "                        \"knowledge_base\":row[\"taxonomy_path\"]\n",
    "                    })\n",
    "            i+=1\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8717057-f292-438a-a8a8-bccca97007b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "\n",
    "if data_type == \"instructlab\":\n",
    "    full_path = f\"{bucket_name}/datasets/labeled/instructlab/{model_name}\"\n",
    "    dataset = load_instruct_lab()\n",
    "    df = pd.DataFrame(dataset)\n",
    "    ### Load test & train datasets\n",
    "    dataframe = Dataset.from_pandas(df).train_test_split(test_size=0.2)\n",
    "    train_dataset= dataframe[\"train\"]\n",
    "    test_dataset=dataframe[\"test\"]\n",
    "else:\n",
    "    full_path = f\"{bucket_name}/datasets/labeled/llm/{model_name}\"\n",
    "    paginator=s3_client.get_paginator('list_objects_v2')\n",
    "    pages=paginator.paginate(Bucket=bucket_name,Prefix=f\"datasets/labeled/gemma/{model_name}\")\n",
    "    i=0\n",
    "    dataset=[]\n",
    "    for page in pages:\n",
    "        page_objects = page.get('Contents', [])\n",
    "        for obj in tqdm(page_objects,desc=\"Loading dataset\"):\n",
    "            data = s3_client.get_object(Bucket=bucket_name, Key=obj.get('Key'))\n",
    "            content =(data['Body'].read()).decode(\"utf-8\")\n",
    "            if(i>0 and type(content) is str):\n",
    "                annotation = json.loads(content)\n",
    "                results=annotation.get(\"result\")\n",
    "                if(len(results) <= 0):\n",
    "                    continue\n",
    "                data = annotation.get(\"task\").get(\"data\")\n",
    "                instruction= data.get(\"input\")\n",
    "                context = data.get(\"input2\")\n",
    "                response = results[0].get(\"value\").get(\"text\")[0]\n",
    "                dataset.append(\n",
    "                    {\n",
    "                        \"instruction\":instruction,\n",
    "                        \"context\":context,\n",
    "                        \"response\":response\n",
    "                    }\n",
    "                )\n",
    "            i+=1\n",
    "    raise Exception(\" Yet not implemented methodology apart from instructlab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0231b871-fabd-4ee9-9334-bea89ba04433",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "first_sample = df.iloc[0]\n",
    "print(f\"Question: {first_sample['messages'][0]['value']}\\n\")\n",
    "print(f\"Answer: {first_sample['messages'][1]['value']}\\n\")\n",
    "print(f\"Knowledge base: {first_sample['knowledge_base']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9c27e9-7200-49b4-86ff-cca2c3b0f753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TrainingArguments,BitsAndBytesConfig,AutoModelForCausalLM\n",
    "\n",
    "def apply_template(examples):\n",
    "    messages = examples[\"messages\"]\n",
    "    text = [tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=False) for message in messages]\n",
    "    return {\"text\": text}\n",
    "    \n",
    "train_dataset = train_dataset.map(apply_template,batched=True)\n",
    "test_dataset = test_dataset.map(apply_template,batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda98109-36c6-4e77-a636-c415af7ea90e",
   "metadata": {},
   "source": [
    "- Batch size refers to the number of training samples processed before the model's internal parameters (weights) are updated.\n",
    "- Gradient accumulation is a technique where gradients are accumulated over several batches before updating the model weights.\n",
    "- This effectively increases the batch size without needing extra memory. For example, if you have a batch size of 2 and accumulate gradients over 4 steps, it is equivalent to having a batch size of 8 in terms of gradient calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d2f198-db5e-4a12-b6cc-e4213817c515",
   "metadata": {
    "tags": []
   },
   "source": [
    "During training, instead of updating the model weights after each batch, the gradients are accumulated over the specified number of batches (accumulation steps). After these steps, the accumulated gradients are used to update the model weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3c0378-da67-4fd9-b662-2aee2245c0aa",
   "metadata": {},
   "source": [
    "The goal of weight decay is to prevent the model from overfitting by discouraging the weights from growing too large, which can help improve generalization on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0228cbfa-4554-459b-b48c-0954c5842144",
   "metadata": {},
   "source": [
    "Warmup steps refer to an initial phase in the training process of a neural network, where the learning rate starts at a lower value and gradually increases to a predetermined maximum value over a set number of iterations or steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8aedfe-fb6c-41a9-938d-771a860b517c",
   "metadata": {},
   "source": [
    "Weight decay It involves adding a penalty to the loss function based on the magnitude of the model's weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f317f00-4d55-4194-81d1-de4d559289ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from trl import SFTTrainer\n",
    "\n",
    "################\n",
    "# Training\n",
    "################\n",
    "training_args = transformers.TrainingArguments(\n",
    "    learning_rate=3e-4,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=2,\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    logging_steps=10,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=10,\n",
    "    output_dir=\"outputs\",\n",
    "    seed=0,\n",
    "    evaluation_strategy=\"steps\",  # Can be \"steps\" or \"epoch\"\n",
    "    eval_steps=10,  # Perform evaluation every n steps\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807299af-44b3-4471-93d7-ad4e4f96e6c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7dc18e-ab11-4414-ae28-8d7533ca3da0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Check CUDA memory\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1908849-2dc3-498f-8e35-bbc930a19bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498500c8-7705-4754-833d-28754c85df10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Check CUDA memory\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5947d589-b1a3-4819-9e38-b0b5eba47f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22547c06-2357-4110-b2d5-d678071a1861",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Flush CUDA memory\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ec4f58-7199-437b-b60d-a24004481223",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Check CUDA memory\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352dec09-7f6a-48c7-9a7f-6aaf777d2d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "messages = [\n",
    "    {\"from\": \"human\", \"value\": \"Which is the best wine to eat with fish??\"},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26404ac-826b-46d1-9aaf-83c189c467e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(inputs, streamer = text_streamer, max_new_tokens = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8257c7f8-45e9-4fe0-9f80-1af11a80e1d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "messages = [\n",
    "    {\"from\": \"human\", \"value\": \"¿Cuál es el tipo de vino apropiado para acompañar el marisco?\"},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d65761b-c7ab-45f1-9847-a3c3c3c39913",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(inputs, streamer = text_streamer, max_new_tokens = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c4e420-317c-4f7a-9eaf-8257f3ff823d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

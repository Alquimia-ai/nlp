{
 "cells": [
  {
   "attachments": {
    "351cec3c-aebb-4b8f-9a35-1011aa689695.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "ed9d52a3-9617-426a-ab32-3fdfe148976e",
   "metadata": {},
   "source": [
    "**LoRA** makes a small change in the fine-tuning process. Instead of computing gradients for all the entries in W (which is huge), and updating all the parameters, it’s algorithm works as follows:\n",
    "- Freeze the original parameters W at the start of fine-tuning.\n",
    "- Decomposes W into two smaller matrices A and B, and update these two small matrices during fine-tuning. The decomposed matrices are significantly smaller in size than the original weight matrix W, and can easily fit into the memory. The decomposition is done in such a way that we can regenerate W from A and B. Basically A and B are LD, so the resulting LI matrix (That it is W) can be composed from A and B\n",
    "- Once fine-tuning is completed, the smaller matrices A and B get adapted for the specific task.\n",
    "- The important thing to note here is that the pretrained weights are not changed during this process as they were frozen by LoRA. The question arises now that how do we use the adapted matrices A and B to do the inference on a new input. Previously with normal finetuning/pretraining, if input to the model was x, the output of layer used to be W.x. But after fine-tuning the model with LoRA, the updated inference equation is:\n",
    "\n",
    "![image.png](attachment:351cec3c-aebb-4b8f-9a35-1011aa689695.png)\n",
    "\n",
    "where 𝑥 is the input, 𝑊 represents model parameters, 𝐵 and 𝐴 represent low-rank adapters trained during LoRA fine-tunin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1661030-b75f-43b7-9b5e-6e086340381d",
   "metadata": {},
   "source": [
    "The amount of memory saved depends on the rank r, a hyperparameter. For instance, if ΔW has 10,000 rows and 20,000 columns, it holds 200,000,000 parameters. Choosing A and B with r=8, A would have 10,000 rows and 8 columns, and B would have 8 rows and 20,000 columns, resulting in 10,000×8 + 8×20,000 = 240,000 parameters, which is approximately 830 times fewer than 200,000,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9050ad03-8ea6-43d7-a479-879344204498",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install peft boto3 bitsandbytes -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f84417-aa82-4890-8b8b-89e7bbbaa6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.device.is_cuda_available():\n",
    "    print(\"Using CUDA cores\")\n",
    "else\n",
    "    print(\"Using CPU cores\")\n",
    "    \n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e50f67-b8ba-4d66-a219-27ce05d36b4d",
   "metadata": {},
   "source": [
    "### The rank in LoRa Fine tuning\n",
    "A higher rank means a greater number of trainable parameters in our model, making fine-tuning more memory intensive. However, higher ranks retain more information from the original weight matrix, as the decomposed matrices themselves are large and capture most of the essence of W (i.e., the model becomes more expressive). We can say that, as the rank increases, LORA essentially converges toward normal fine-tuning.\n",
    "### Alpha in LoRa\n",
    "A higher “alpha” would place more emphasis on the low-rank structure or regularization, while a lower “alpha” would reduce its influence, making the model rely more on the original parameters. Adjusting “alpha” helps in striking a balance between fitting the data and preventing overfitting by regularizing the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec85936-8e3f-4f47-b817-2bc6fcb03dbb",
   "metadata": {},
   "source": [
    "As a rule of thumb, it’s usually common to choose an alpha that is twice as large as the rank when fine-tuning LLMs (note that this is different when working with diffusion models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e298f1c5-6c93-4fb5-808b-8bbe1d40a32f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "import os\n",
    "\n",
    "### A sweet pot for LoRa is r=8\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_alpha=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817086f9-9c14-4adc-8bed-a0c458335821",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    " \n",
    "try:\n",
    "    p = getpass.getpass()\n",
    "except Exception as error:\n",
    "    print('ERROR', error)\n",
    "else:\n",
    "    print('Hf token set succesfully')\n",
    "    os.environ[\"HF_TOKEN\"]= p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2743d929-bf6c-4335-b2e9-e8391428f6e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "s3_endpoint = os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "s3_secret = os.environ[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "s3_access_key = os.environ[\"AWS_ACCESS_KEY_ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6afdad-b1b4-4bde-b447-78e9c8aa8618",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "## Load dataset\n",
    "if s3_access_key is None: raise TypeError(f\"'S3_ACCESS_KEY_ID' env variable is not set\")\n",
    "if s3_secret is None: raise TypeError(f\"'S3_SECRET_ACCESS_KEY' env variable is not set\")\n",
    "\n",
    "# Enter the bucket and model name\n",
    "bucket_name = input(\"Enter bucket name:\")\n",
    "model_name = input(\"Enter model name:\")\n",
    "\n",
    "if bucket_name == \"\": raise TypeError(f\"'bucket_name' input variable is empty\")\n",
    "if model_name == \"\": raise TypeError(f\"'model_name' input variable is empty\")\n",
    "\n",
    "full_path = f\"{bucket_name}/datasets/labeled/gemma/{model_name}\"\n",
    "\n",
    "print(\"\\nConnecting to S3 in path: \"+ full_path)\n",
    "\n",
    "# Initialize session and client with boto3\n",
    "session = boto3.session.Session()\n",
    "s3_client = session.client('s3',\n",
    "                           region_name='nyc3',\n",
    "                           endpoint_url=s3_endpoint,\n",
    "                           aws_access_key_id=s3_access_key,\n",
    "                           aws_secret_access_key=s3_secret\n",
    "                          )\n",
    "# Initialize paginator\n",
    "paginator=s3_client.get_paginator('list_objects_v2')\n",
    "pages=paginator.paginate(Bucket=bucket_name,Prefix=f\"datasets/labeled/gemma/{model_name}\")\n",
    "i=0\n",
    "dataset=[]\n",
    "for page in pages:\n",
    "    page_objects = page.get('Contents', [])\n",
    "    for obj in tqdm(page_objects,desc=\"Loading dataset\"):\n",
    "        data = s3_client.get_object(Bucket=bucket_name, Key=obj.get('Key'))\n",
    "        content =(data['Body'].read()).decode(\"utf-8\")\n",
    "        if(i>0 and type(content) is str):\n",
    "            annotation = json.loads(content)\n",
    "            results=annotation.get(\"result\")\n",
    "            if(len(results) <= 0):\n",
    "                continue\n",
    "            data = annotation.get(\"task\").get(\"data\")\n",
    "            instruction= data.get(\"input\")\n",
    "            context = data.get(\"input2\")\n",
    "            response = results[0].get(\"value\").get(\"text\")[0]\n",
    "            dataset.append(\n",
    "                {\n",
    "                    \"instruction\":instruction,\n",
    "                    \"context\":context,\n",
    "                    \"response\":response\n",
    "                }\n",
    "            )\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3b9a98-a3e4-41c9-b71f-e63724caa422",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6497a3c-f9bf-4f21-b682-239a42b2d1d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Parse to dataframe from pandas and show first 3\n",
    "df = pd.DataFrame(dataset)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852e383c-3e92-4e45-bd73-23ebc6340dce",
   "metadata": {},
   "source": [
    "### Check how is performing the base model without PEFT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526ebb82-e964-4295-b442-fef4c511fa84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"google/gemma-2b\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=os.environ['HF_TOKEN'])\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0}, token=os.environ['HF_TOKEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a6d8d4-6ec8-47a5-8baa-5b80f9be7085",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = df.iloc[0]\n",
    "instruction = sample[\"instruction\"]\n",
    "context = sample[\"context\"]\n",
    "print(\"Context:\\n\")\n",
    "print(f\"{context}\\n\")\n",
    "\n",
    "print(\"Instructions:\\n\")\n",
    "print(f\"{instruction}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166a8172-e106-4ee9-b229-90f387b74582",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = f\"Instructions:{instruction}\\nContext:{context}\\nResponse:\"\n",
    "device = \"cuda:0\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(**inputs,max_length=1000)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e128251-b461-4f02-917a-64d5d831e719",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"EXPECTED ANSWER:\\n\")\n",
    "print(f\"{sample['response']}\\n\")\n",
    "\n",
    "print(f\"ANSWER:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d46c201-93a8-4c35-a643-97583e4433a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from trl import SFTTrainer\n",
    "\n",
    "def formatting_func(example):\n",
    "    ## Format text according dataset\n",
    "    text = f\"Instructions:{example[\"instruction\"]}\\nContext:{example[\"context\"]}\\nResponse:{example[\"response\"]}<eos>\"\n",
    "    return [text]\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=df,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=2,\n",
    "        max_steps=10,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\"\n",
    "    ),\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=formatting_func,\n",
    ")\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
